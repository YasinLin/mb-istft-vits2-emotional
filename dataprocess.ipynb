{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9842d428-783a-4038-a89a-c4b2dcccf6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0001': 0, '0002': 1, '0003': 2, '0004': 3, '0005': 4, '0006': 5, '0007': 6, '0008': 7, '0009': 8, '0010': 9, '0011': 10, '0012': 11, '0013': 12, '0014': 13, '0015': 14, '0016': 15, '0017': 16, '0018': 17, '0019': 18, '0020': 19}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "emo_dict={\n",
    "    \"生气\": 'Angry',\n",
    "    \"快乐\": 'Happy',\n",
    "    \"中立\": 'Neutral',\n",
    "    \"伤心\": 'Sad',\n",
    "    \"惊喜\": 'Surprise'\n",
    "}\n",
    "\n",
    "#读取说话人\n",
    "esd_spk_dict = {}\n",
    "esd_spk_info_dir = \"/data/datasets/ESD\"\n",
    "index = 0\n",
    "\n",
    "for item in sorted(os.listdir(esd_spk_info_dir)):\n",
    "    item_path = os.path.join(esd_spk_info_dir, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        esd_spk_dict[item] = index\n",
    "        index+=1\n",
    "          \n",
    "print(esd_spk_dict)\n",
    "\n",
    "\n",
    "def resample(f, output_file):\n",
    "    wav, sr = librosa.load(f)\n",
    "    y_resampled = librosa.resample(wav, sr, 16000)\n",
    "    sf.write(output_file, y_resampled, 16000)\n",
    "\n",
    "\n",
    "def gen_esd_filelist(rs = False):\n",
    "    all={}\n",
    "    all_rand = {}\n",
    "    with open(f'filelists/esd_train.txt', 'w') as tfile:\n",
    "        with open(f'filelists/esd_val.txt', 'w') as vfile:\n",
    "            for item in esd_spk_dict:\n",
    "                item_path = os.path.join(esd_spk_info_dir, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    text_path = os.path.join(item_path, f'{item}.txt')\n",
    "                    with open (text_path, 'r') as file:\n",
    "                        line = file.readline()\n",
    "                        while line:\n",
    "                            try:\n",
    "                                arr = line.strip().split(\"\\t\")\n",
    "                                if len(arr) == 3:\n",
    "                                    name = arr[0]\n",
    "                                    spk_name = item\n",
    "                                    spk_id = esd_spk_dict[spk_name]\n",
    "                                    if spk_id>=10:\n",
    "                                        continue\n",
    "                                    text = arr[1]\n",
    "                                    if arr[2] in emo_dict:\n",
    "                                        subdir = emo_dict[arr[2]]\n",
    "                                    else:\n",
    "                                        subdir = arr[2]\n",
    "                                    all_key = str(spk_id)+\"_\"+subdir\n",
    "                                    f = f'{esd_spk_info_dir}/{item}/{subdir}/{name}.wav'\n",
    "                                    if all_key not in all:\n",
    "                                        all[all_key] = 0\n",
    "                                        all_rand[all_key] = random.randint(0, 703)\n",
    "    \n",
    "                                    ls = [f, spk_id, text]\n",
    "                                    if all[all_key] == all_rand[all_key]:\n",
    "                                        vfile.write('|'.join([str(s) for s in ls])+\"\\n\")\n",
    "                                    else:\n",
    "                                        tfile.write('|'.join([str(s) for s in ls])+\"\\n\")\n",
    "    \n",
    "                                    all[all_key] = all[all_key] + 1\n",
    "                            finally:\n",
    "                                line = file.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99f19b74-4c9b-4457-bf21-1e6b45aca897",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_esd_filelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53912a6a-55f7-41b1-a857-899d15af330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SSB1837': 20, 'SSB0578': 21, 'SSB1216': 22, 'SSB1161': 23, 'SSB0016': 24, 'SSB1365': 25, 'SSB1759': 26, 'SSB0588': 27, 'SSB0534': 28, 'SSB0380': 29, 'SSB0273': 30, 'SSB1863': 31, 'SSB1125': 32, 'SSB1872': 33, 'SSB0993': 34, 'SSB0666': 35, 'SSB0668': 36, 'SSB0395': 37, 'SSB1831': 38, 'SSB1064': 39, 'SSB1219': 40, 'SSB1322': 41, 'SSB1002': 42, 'SSB1878': 43, 'SSB0415': 44, 'SSB1781': 45, 'SSB0631': 46, 'SSB0686': 47, 'SSB1328': 48, 'SSB0366': 49, 'SSB1100': 50, 'SSB0241': 51, 'SSB0966': 52, 'SSB1340': 53, 'SSB0762': 54, 'SSB0073': 55, 'SSB0632': 56, 'SSB0915': 57, 'SSB0748': 58, 'SSB1956': 59, 'SSB1056': 60, 'SSB0716': 61, 'SSB0629': 62, 'SSB1806': 63, 'SSB0599': 64, 'SSB0720': 65, 'SSB0385': 66, 'SSB0809': 67, 'SSB0342': 68, 'SSB0760': 69, 'SSB1253': 70, 'SSB1575': 71, 'SSB0863': 72, 'SSB1110': 73, 'SSB0200': 74, 'SSB1215': 75, 'SSB0375': 76, 'SSB1828': 77, 'SSB0737': 78, 'SSB0341': 79, 'SSB0009': 80, 'SSB0309': 81, 'SSB1055': 82, 'SSB1448': 83, 'SSB1176': 84, 'SSB1001': 85, 'SSB0193': 86, 'SSB0710': 87, 'SSB0427': 88, 'SSB0338': 89, 'SSB1131': 90, 'SSB1108': 91, 'SSB0149': 92, 'SSB0736': 93, 'SSB1555': 94, 'SSB0614': 95, 'SSB1072': 96, 'SSB1728': 97, 'SSB1382': 98, 'SSB0851': 99, 'SSB1585': 100, 'SSB1891': 101, 'SSB1393': 102, 'SSB1274': 103, 'SSB1204': 104, 'SSB1452': 105, 'SSB0570': 106, 'SSB0780': 107, 'SSB1593': 108, 'SSB0913': 109, 'SSB1302': 110, 'SSB0323': 111, 'SSB1135': 112, 'SSB0382': 113, 'SSB0887': 114, 'SSB1625': 115, 'SSB1366': 116, 'SSB0693': 117, 'SSB0594': 118, 'SSB1686': 119, 'SSB0012': 120, 'SSB0139': 121, 'SSB0751': 122, 'SSB0606': 123, 'SSB1341': 124, 'SSB0145': 125, 'SSB1136': 126, 'SSB0339': 127, 'SSB0482': 128, 'SSB0502': 129, 'SSB1650': 130, 'SSB0817': 131, 'SSB0261': 132, 'SSB0316': 133, 'SSB0033': 134, 'SSB0723': 135, 'SSB1008': 136, 'SSB0700': 137, 'SSB1457': 138, 'SSB0601': 139, 'SSB1809': 140, 'SSB1739': 141, 'SSB0407': 142, 'SSB0426': 143, 'SSB0470': 144, 'SSB0935': 145, 'SSB0822': 146, 'SSB0746': 147, 'SSB0758': 148, 'SSB1221': 149, 'SSB0038': 150, 'SSB1624': 151, 'SSB0133': 152, 'SSB0778': 153, 'SSB0702': 154, 'SSB1383': 155, 'SSB1563': 156, 'SSB1670': 157, 'SSB1096': 158, 'SSB0299': 159, 'SSB1711': 160, 'SSB1810': 161, 'SSB1115': 162, 'SSB1684': 163, 'SSB1402': 164, 'SSB1918': 165, 'SSB0246': 166, 'SSB1607': 167, 'SSB1437': 168, 'SSB0011': 169, 'SSB0288': 170, 'SSB0539': 171, 'SSB0394': 172, 'SSB0379': 173, 'SSB1187': 174, 'SSB0671': 175, 'SSB0544': 176, 'SSB0005': 177, 'SSB1846': 178, 'SSB0122': 179, 'SSB1630': 180, 'SSB1399': 181, 'SSB1567': 182, 'SSB0267': 183, 'SSB1392': 184, 'SSB0287': 185, 'SSB0717': 186, 'SSB0018': 187, 'SSB0315': 188, 'SSB1126': 189, 'SSB1320': 190, 'SSB0919': 191, 'SSB0623': 192, 'SSB0871': 193, 'SSB0786': 194, 'SSB1024': 195, 'SSB1935': 196, 'SSB0794': 197, 'SSB1832': 198, 'SSB1000': 199, 'SSB1431': 200, 'SSB0535': 201, 'SSB1782': 202, 'SSB0354': 203, 'SSB0393': 204, 'SSB0057': 205, 'SSB1385': 206, 'SSB1050': 207, 'SSB0435': 208, 'SSB1197': 209, 'SSB1939': 210, 'SSB1239': 211, 'SSB0434': 212, 'SSB1020': 213, 'SSB1091': 214, 'SSB1745': 215, 'SSB0565': 216, 'SSB0711': 217, 'SSB0603': 218, 'SSB0749': 219, 'SSB0997': 220, 'SSB0590': 221, 'SSB0784': 222, 'SSB1699': 223, 'SSB0607': 224, 'SSB1902': 225, 'SSB0043': 226, 'SSB1138': 227, 'SSB1408': 228, 'SSB1218': 229, 'SSB1377': 230, 'SSB0609': 231, 'SSB0987': 232, 'SSB0080': 233, 'SSB0307': 234, 'SSB0197': 235, 'SSB1203': 236, 'SSB0112': 237, 'SSB1957': 238}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "\n",
    "#读取说话人\n",
    "spk_dict = {}\n",
    "spk_info_file='/data/datasets/data_aishell3/spk-info.txt'\n",
    "with open(spk_info_file, 'r') as file:\n",
    "    # 执行所需的操作，比如读取文件内容\n",
    "    lines = file.readlines()\n",
    "    index = 20\n",
    "    for (i, line) in enumerate(lines):\n",
    "        arr = line.strip().split(\"\\t\")\n",
    "        if len(arr) == 4:\n",
    "            spk_dict[arr[0]] = index\n",
    "            index+=1\n",
    "          \n",
    "print(spk_dict)\n",
    "\n",
    "\n",
    "def resample(f, output_file):\n",
    "    wav, sr = librosa.load(f)\n",
    "    y_resampled = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "    sf.write(output_file, y_resampled, 16000)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def gen_filelist_by_dataset(dataset = None, rs = False):\n",
    "    datasets = ['test', 'train']\n",
    "    if dataset != None:\n",
    "        datasets = [dataset]\n",
    "    print(datasets)\n",
    "    #构建filelist\n",
    "    for dataset in datasets:\n",
    "        with open(f'filelists/aishell3_{dataset}.txt', 'w') as nfile:\n",
    "            file_path = f'/data/datasets/data_aishell3/{dataset}/content.txt'\n",
    "            with open(file_path, 'r') as file:\n",
    "                # 执行所需的操作，比如读取文件内容\n",
    "                line = file.readline()\n",
    "                while line:\n",
    "                    arr = line.strip().split(\"\\t\")\n",
    "                    if len(arr) == 2:\n",
    "                        # 处理每一行的逻辑\n",
    "                        name = arr[0]\n",
    "                        spk_name = name[:7]\n",
    "                        spk_id = spk_dict[spk_name]\n",
    "                        text = ''.join(arr[1].split(\" \")[::2])\n",
    "                        f = f'/data/datasets/data_aishell3/{dataset}/wav/{spk_name}/{name}'\n",
    "                        if rs:\n",
    "                            output_file = f'dataset/aishell3_16k/{name}'\n",
    "                            if not os.path.isfile(output_file):\n",
    "                                resample(f, output_file)\n",
    "                        else:\n",
    "                            output_file = f\n",
    "                        # print(name,f,spk_name,spk_id,text) # 打印去除换行符的行内容\n",
    "                        ls = [output_file, spk_id, text]\n",
    "                        nfile.write('|'.join([str(s) for s in ls])+\"\\n\")\n",
    "                    line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e38464-ac38-46d7-b4a8-a4679d3bb39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n"
     ]
    }
   ],
   "source": [
    "gen_filelist_by_dataset('test', rs= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1285bb0-90e2-4d16-b104-426536cd6fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train']\n"
     ]
    }
   ],
   "source": [
    "gen_filelist_by_dataset('train', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e841767-5049-49a5-8f14-9118c97c4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(file1, file2, output_file):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2, open(output_file, 'w') as out_file:\n",
    "        for line in f1:\n",
    "            out_file.write(line.strip() + \"\\n\")\n",
    "        for line in f2:\n",
    "            out_file.write(line.strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f596adc-110e-4499-aa49-fcbad61d689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files('filelists/esd_val.txt','filelists/aishell3_val.txt', \"filelists/esd_aishell3_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bab4211-ffac-403c-941b-27e93884b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files('filelists/esd_train.txt','filelists/aishell3_train.txt', \"filelists/esd_aishell3_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b20e6c6-f662-4fe6-9bdd-92ca1fdb68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 tensor 是一个形状为 [16, 18, 192] 的张量\n",
    "tensor = torch.randn(16, 18, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65529541-9ed1-4d82-aeb1-ea237eddf7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 18, 192])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f23292-17ac-4902-acac-48fe0e3153ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (119) must match the size of tensor b (18) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m119\u001b[39m, \u001b[38;5;241m192\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m192\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (119) must match the size of tensor b (18) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a = torch.randn(16, 119, 192).transpose(1, 2)\n",
    "b = torch.randn(16, 18, 192)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c35d9f-555b-4fd3-9076-bd206604f88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 1, 192]' is invalid for input of size 55296",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 1, 192]' is invalid for input of size 55296"
     ]
    }
   ],
   "source": [
    "print(tensor.reshape(16,1,192).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc359c-aa2e-439d-aae6-32f95600035b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
